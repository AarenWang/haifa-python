#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•ç»“æœåˆ†æä¸æŠ¥å‘Šç”Ÿæˆå™¨
"""

import json
import sys
import statistics
from pathlib import Path
from typing import Dict, List

def load_results(results_file: str) -> Dict:
    """åŠ è½½æµ‹è¯•ç»“æœ"""
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Results file '{results_file}' not found.")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON in results file: {e}")
        sys.exit(1)

def generate_performance_report(results: Dict) -> str:
    """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
    report_lines = []
    
    # æ ‡é¢˜
    report_lines.append("# Lua è§£é‡Šå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
    
    # æµ‹è¯•ç¯å¢ƒä¿¡æ¯
    test_info = results.get("test_info", {})
    report_lines.append("## æµ‹è¯•ç¯å¢ƒä¿¡æ¯\n")
    report_lines.append(f"- **æµ‹è¯•æ—¶é—´**: {test_info.get('timestamp', 'Unknown')}")
    report_lines.append(f"- **æµ‹è¯•è¿­ä»£**: {test_info.get('iterations', 'Unknown')} æ¬¡")
    
    if test_info.get('lua_version'):
        report_lines.append(f"- **Lua ç‰ˆæœ¬**: {test_info.get('lua_version')}")
    else:
        report_lines.append("- **Lua ç‰ˆæœ¬**: æœªæ£€æµ‹åˆ°å®˜æ–¹ Lua è§£é‡Šå™¨")
    
    report_lines.append(f"- **Python ç‰ˆæœ¬**: {test_info.get('python_version', 'Unknown')}")
    
    system_info = test_info.get('system_info', {})
    if system_info:
        report_lines.append(f"- **æ“ä½œç³»ç»Ÿ**: {system_info.get('platform', 'Unknown')}")
        report_lines.append(f"- **å¤„ç†å™¨**: {system_info.get('processor', 'Unknown')}")
    
    report_lines.append("")
    
    # æ€§èƒ½å¯¹æ¯”æ€»è§ˆ
    report_lines.append("## æ€§èƒ½å¯¹æ¯”æ€»è§ˆ\n")
    
    tests = results.get("tests", {})
    if not tests:
        report_lines.append("âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ•°æ®ã€‚\n")
        return "\n".join(report_lines)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨
    report_lines.append("| æµ‹è¯•é¡¹ç›® | Lua å®˜æ–¹ (ç§’) | haifa_lua (ç§’) | æ€§èƒ½æ¯”ç‡ | ç›¸å¯¹æ€§èƒ½ |")
    report_lines.append("|----------|---------------|----------------|----------|----------|")
    
    ratios = []
    performances = []
    
    for test_name, test_data in tests.items():
        lua_data = test_data.get("lua_official", {})
        haifa_data = test_data.get("haifa_lua", {})
        
        lua_time = lua_data.get("avg_time", 0)
        haifa_time = haifa_data.get("avg_time", 0)
        ratio = test_data.get("performance_ratio", 0)
        rel_perf = test_data.get("relative_performance", 0)
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        lua_str = f"{lua_time:.4f}" if lua_time > 0 else "N/A"
        haifa_str = f"{haifa_time:.4f}" if haifa_time > 0 else "N/A"
        ratio_str = f"{ratio:.2f}x" if ratio > 0 else "N/A"
        perf_str = f"{rel_perf:.1f}%" if rel_perf > 0 else "N/A"
        
        # ç®€åŒ–æµ‹è¯•åç§°æ˜¾ç¤º
        display_name = test_name.replace("_bench.lua", "").replace("_", " ").title()
        
        report_lines.append(f"| {display_name} | {lua_str} | {haifa_str} | {ratio_str} | {perf_str} |")
        
        if ratio > 0:
            ratios.append(ratio)
            performances.append(rel_perf)
    
    # è®¡ç®—æ€»ä½“æ€§èƒ½
    if ratios:
        avg_ratio = statistics.mean(ratios)
        avg_performance = statistics.mean(performances)
        
        report_lines.append(f"| **å¹³å‡å€¼** | - | - | **{avg_ratio:.2f}x** | **{avg_performance:.1f}%** |")
        report_lines.append("")
        
        # æ€§èƒ½è¯„ä¼°
        report_lines.append("### æ€»ä½“æ€§èƒ½è¯„ä¼°\n")
        
        if avg_performance >= 70:
            level = "ğŸŸ¢ ä¼˜ç§€"
            assessment = "haifa_lua çš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå¯ä»¥æ»¡è¶³å¤§éƒ¨åˆ†ç”Ÿäº§ç¯å¢ƒéœ€æ±‚ã€‚"
        elif avg_performance >= 50:
            level = "ğŸŸ¡ è‰¯å¥½" 
            assessment = "haifa_lua çš„æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œé€‚åˆå¤§å¤šæ•°åº”ç”¨åœºæ™¯ï¼Œéƒ¨åˆ†é«˜æ€§èƒ½åœºæ™¯å¯èƒ½éœ€è¦ä¼˜åŒ–ã€‚"
        elif avg_performance >= 30:
            level = "ğŸŸ  ä¸€èˆ¬"
            assessment = "haifa_lua çš„æ€§èƒ½è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®åœ¨æ€§èƒ½æ•æ„Ÿçš„åº”ç”¨ä¸­è°¨æ…ä½¿ç”¨ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚"
        else:
            level = "ğŸ”´ è¾ƒå·®"
            assessment = "haifa_lua çš„æ€§èƒ½è¡¨ç°è¾ƒå·®ï¼Œä¸å»ºè®®ç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œéœ€è¦å¤§å¹…ä¼˜åŒ–ã€‚"
        
        report_lines.append(f"**æ€§èƒ½ç­‰çº§**: {level}")
        report_lines.append(f"**å¹³å‡æ€§èƒ½**: {avg_performance:.1f}% (ç›¸å¯¹äºå®˜æ–¹ Lua)")
        report_lines.append(f"**è¯„ä¼°ç»“è®º**: {assessment}")
        report_lines.append("")
    else:
        report_lines.append("\nâš ï¸ æ— æ³•è®¡ç®—æ€§èƒ½æ¯”ç‡ï¼Œå¯èƒ½æ˜¯ç”±äºæµ‹è¯•æ‰§è¡Œå¤±è´¥ã€‚\n")
    
    # è¯¦ç»†æµ‹è¯•ç»“æœ
    report_lines.append("## è¯¦ç»†æµ‹è¯•ç»“æœ\n")
    
    for test_name, test_data in tests.items():
        display_name = test_name.replace("_bench.lua", "").replace("_", " ").title()
        report_lines.append(f"### {display_name}\n")
        
        # æµ‹è¯•å®Œæˆæƒ…å†µ
        iterations = test_data.get("iterations_completed", {})
        lua_completed = iterations.get("lua", 0)
        haifa_completed = iterations.get("haifa", 0)
        total_iterations = test_info.get("iterations", 0)
        
        report_lines.append(f"**æµ‹è¯•å®Œæˆæƒ…å†µ**:")
        report_lines.append(f"- Lua å®˜æ–¹: {lua_completed}/{total_iterations} æ¬¡æˆåŠŸ")
        report_lines.append(f"- haifa_lua: {haifa_completed}/{total_iterations} æ¬¡æˆåŠŸ")
        report_lines.append("")
        
        # Lua å®˜æ–¹ç»“æœ
        lua_data = test_data.get("lua_official", {})
        if lua_data:
            report_lines.append("**Lua å®˜æ–¹è§£é‡Šå™¨ç»“æœ**:")
            report_lines.append(f"- å¹³å‡æ‰§è¡Œæ—¶é—´: {lua_data.get('avg_time', 0):.4f} ç§’")
            report_lines.append(f"- æœ€å¿«æ—¶é—´: {lua_data.get('min_time', 0):.4f} ç§’")
            report_lines.append(f"- æœ€æ…¢æ—¶é—´: {lua_data.get('max_time', 0):.4f} ç§’")
            
            std_dev = lua_data.get('std_dev', 0)
            if std_dev > 0:
                report_lines.append(f"- æ ‡å‡†å·®: {std_dev:.4f} ç§’")
            
            # æ˜¾ç¤ºæ ·æœ¬è¾“å‡ºï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰
            sample_output = lua_data.get('sample_output', '').strip()
            if sample_output:
                lines = sample_output.split('\n')[:3]  # æ˜¾ç¤ºå‰3è¡Œ
                output_preview = '\n'.join(f"  {line}" for line in lines)
                report_lines.append(f"- è¾“å‡ºç¤ºä¾‹:\n```\n{output_preview}")
                if len(sample_output.split('\n')) > 3:
                    report_lines.append("  ...")
                report_lines.append("```")
            report_lines.append("")
        
        # haifa_lua ç»“æœ
        haifa_data = test_data.get("haifa_lua", {})
        if haifa_data:
            report_lines.append("**haifa_lua è§£é‡Šå™¨ç»“æœ**:")
            report_lines.append(f"- å¹³å‡æ‰§è¡Œæ—¶é—´: {haifa_data.get('avg_time', 0):.4f} ç§’")
            report_lines.append(f"- æœ€å¿«æ—¶é—´: {haifa_data.get('min_time', 0):.4f} ç§’")
            report_lines.append(f"- æœ€æ…¢æ—¶é—´: {haifa_data.get('max_time', 0):.4f} ç§’")
            
            std_dev = haifa_data.get('std_dev', 0)
            if std_dev > 0:
                report_lines.append(f"- æ ‡å‡†å·®: {std_dev:.4f} ç§’")
            
            # æ˜¾ç¤ºæ ·æœ¬è¾“å‡º
            sample_output = haifa_data.get('sample_output', '').strip()
            if sample_output:
                lines = sample_output.split('\n')[:3]
                output_preview = '\n'.join(f"  {line}" for line in lines)
                report_lines.append(f"- è¾“å‡ºç¤ºä¾‹:\n```\n{output_preview}")
                if len(sample_output.split('\n')) > 3:
                    report_lines.append("  ...")
                report_lines.append("```")
            report_lines.append("")
        
        # æ€§èƒ½å¯¹æ¯”åˆ†æ
        ratio = test_data.get("performance_ratio", 0)
        rel_perf = test_data.get("relative_performance", 0)
        
        if ratio > 0 and rel_perf > 0:
            report_lines.append("**æ€§èƒ½åˆ†æ**:")
            report_lines.append(f"- æ€§èƒ½æ¯”ç‡: {ratio:.2f}x (haifa_lua ç›¸å¯¹äº Lua å®˜æ–¹çš„å€æ•°)")
            report_lines.append(f"- ç›¸å¯¹æ€§èƒ½: {rel_perf:.1f}% (haifa_lua è¾¾åˆ° Lua å®˜æ–¹çš„æ€§èƒ½ç™¾åˆ†æ¯”)")
            
            if ratio < 2.0:
                analysis = "æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œæ¥è¿‘å®˜æ–¹è§£é‡Šå™¨æ°´å¹³"
            elif ratio < 4.0:
                analysis = "æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…"
            elif ratio < 8.0:
                analysis = "æ€§èƒ½è¡¨ç°ä¸€èˆ¬ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´"
            else:
                analysis = "æ€§èƒ½è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–"
            
            report_lines.append(f"- è¯„ä¼°: {analysis}")
            report_lines.append("")
        
        report_lines.append("---\n")
    
    # ä¼˜åŒ–å»ºè®®
    if ratios:
        report_lines.append("## æ€§èƒ½ä¼˜åŒ–å»ºè®®\n")
        
        # æ‰¾å‡ºæ€§èƒ½æœ€å·®çš„æµ‹è¯•
        worst_tests = []
        for test_name, test_data in tests.items():
            ratio = test_data.get("performance_ratio", 0)
            if ratio > 0:
                worst_tests.append((test_name, ratio))
        
        worst_tests.sort(key=lambda x: x[1], reverse=True)
        
        if worst_tests:
            report_lines.append("### ä¼˜å…ˆä¼˜åŒ–é¡¹ç›®\n")
            for i, (test_name, ratio) in enumerate(worst_tests[:3]):
                display_name = test_name.replace("_bench.lua", "").replace("_", " ")
                report_lines.append(f"{i+1}. **{display_name}**: {ratio:.2f}x å€æ€§èƒ½å·®è·")
        
        report_lines.append("\n### é€šç”¨ä¼˜åŒ–ç­–ç•¥\n")
        report_lines.append("1. **æŒ‡ä»¤ä¼˜åŒ–**: å‡å°‘å­—èŠ‚ç æŒ‡ä»¤æ•°é‡ï¼Œä¼˜åŒ–å¸¸ç”¨æ“ä½œè·¯å¾„")
        report_lines.append("2. **å†…å­˜ç®¡ç†**: ä¼˜åŒ–å¯¹è±¡åˆ†é…å’Œåƒåœ¾å›æ”¶ç­–ç•¥")  
        report_lines.append("3. **å‡½æ•°è°ƒç”¨**: å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€ï¼Œä¼˜åŒ–æ ˆå¸§ç®¡ç†")
        report_lines.append("4. **ç¼“å­˜ç­–ç•¥**: ç¼“å­˜å¸¸ç”¨è®¡ç®—ç»“æœå’ŒæŸ¥æ‰¾æ“ä½œ")
        report_lines.append("5. **JIT ç¼–è¯‘**: è€ƒè™‘ä¸ºçƒ­ç‚¹ä»£ç å®ç°å³æ—¶ç¼–è¯‘")
        report_lines.append("")
    
    return "\n".join(report_lines)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("Usage: python analyze_results.py <results_file.json>")
        print("\nExample:")
        print("  python analyze_results.py benchmark/results/benchmark_results_20231225_143022.json")
        sys.exit(1)
    
    results_file = sys.argv[1]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(results_file).exists():
        print(f"Error: File '{results_file}' not found.")
        
        # æä¾›å¸®åŠ©ä¿¡æ¯
        results_dir = Path("benchmark/results")
        if results_dir.exists():
            result_files = list(results_dir.glob("benchmark_results_*.json"))
            if result_files:
                print("\nAvailable result files:")
                for file in sorted(result_files, reverse=True)[:5]:
                    print(f"  {file}")
        sys.exit(1)
    
    print(f"Analyzing results from: {results_file}")
    
    # åŠ è½½å¹¶åˆ†æç»“æœ
    results = load_results(results_file)
    report = generate_performance_report(results)
    
    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*80)
    print(report)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    output_file = Path(results_file).with_suffix('.md')
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\n{'='*80}")
    print(f"Report saved to: {output_file}")

if __name__ == "__main__":
    main()